{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss, accuracy_score, classification_report, f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(r'F:\\Projects\\Cap_github\\Capstone\\Data\\DDoS_Recon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].apply(lambda x: 0 if 'DDoS' in str(x) else 1)  # Use -1 for unmatched labels if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['label'],axis=1)\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2, shuffle=True,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant features and select important features\n",
    "def Feature_Importance_LGBM(data):\n",
    "    features = data.drop(['label'],axis=1).values  # \"label\" should be changed to the target class variable name if different\n",
    "    labels = data['label'].values\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = list(data.drop(['label'],axis=1).columns)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    model = lgb.LGBMRegressor(verbose = -1)\n",
    "    model.fit(features, labels)\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': model.feature_importances_})\n",
    "\n",
    "    # Sort features according to importance\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "\n",
    "    # Normalize the feature importances to add up to one\n",
    "    feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n",
    "    feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n",
    "\n",
    "    cumulative_importance=0.90 # Only keep the important features with cumulative importance scores>=90%. It can be changed.\n",
    "\n",
    "    # Make sure most important features are on top\n",
    "    feature_importances = feature_importances.sort_values('cumulative_importance')\n",
    "\n",
    "    # Identify the features not needed to reach the cumulative_importance\n",
    "    record_low_importance = feature_importances[feature_importances['cumulative_importance'] > cumulative_importance]\n",
    "\n",
    "    to_drop = list(record_low_importance['feature'])\n",
    "    print(feature_importances.drop(['importance'],axis=1))\n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant features\n",
    "def Feature_Redundancy_Pearson(data):\n",
    "    correlation_threshold=0.95 # Only remove features with the redundancy>90%. It can be changed\n",
    "    features = data.drop(['label'],axis=1)\n",
    "    corr_matrix = features.corr()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(bool))\n",
    "\n",
    "    # Select the features with correlations above the threshold\n",
    "    # Need to use the absolute value\n",
    "    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "\n",
    "    # Dataframe to hold correlated pairs\n",
    "    record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n",
    "\n",
    "    # Iterate through the columns to drop\n",
    "    for column in to_drop:\n",
    "\n",
    "        # Find the correlated features\n",
    "        corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n",
    "\n",
    "        # Find the correlated values\n",
    "        corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n",
    "        drop_features = [column for _ in range(len(corr_features))]\n",
    "\n",
    "        # Record the information (need a temp df for now)\n",
    "        temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n",
    "                                         'corr_feature': corr_features,\n",
    "                                         'corr_value': corr_values})\n",
    "        record_collinear = pd.concat([record_collinear, temp_df], ignore_index = True)\n",
    "#     print(record_collinear)\n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Auto_Feature_Engineering(s1):\n",
    "    drop1 = Feature_Importance_LGBM(s1)\n",
    "    df1 = s1.drop(columns = drop1)\n",
    "\n",
    "    drop4 = Feature_Redundancy_Pearson(df1)\n",
    "    df4 = df1.drop(columns = drop4)\n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            feature  normalized_importance  cumulative_importance\n",
      "0               IAT               0.192667               0.192667\n",
      "1          Duration               0.100000               0.292667\n",
      "2         syn_count               0.066667               0.359333\n",
      "3               Min               0.063000               0.422333\n",
      "4            Number               0.060000               0.482333\n",
      "5          Variance               0.044333               0.526667\n",
      "6          Tot_size               0.044000               0.570667\n",
      "7         rst_count               0.040667               0.611333\n",
      "8              HTTP               0.040000               0.651333\n",
      "9              Rate               0.038000               0.689333\n",
      "10        urg_count               0.033667               0.723000\n",
      "11  psh_flag_number               0.032000               0.755000\n",
      "12    flow_duration               0.029667               0.784667\n",
      "13          Tot_sum               0.026000               0.810667\n",
      "14    Protocol_Type               0.024000               0.834667\n",
      "15           Weight               0.023333               0.858000\n",
      "16       Covariance               0.019000               0.877000\n",
      "17    Header_Length               0.019000               0.896000\n",
      "18              Std               0.018000               0.914000\n",
      "19        fin_count               0.017333               0.931333\n",
      "20         Magnitue               0.015667               0.947000\n",
      "21              AVG               0.015000               0.962000\n",
      "22              Max               0.013333               0.975333\n",
      "23        ack_count               0.009667               0.985000\n",
      "24             ICMP               0.004333               0.989333\n",
      "25           Radius               0.003667               0.993000\n",
      "26  rst_flag_number               0.001667               0.994667\n",
      "27            HTTPS               0.001333               0.996000\n",
      "28  ack_flag_number               0.001000               0.997000\n",
      "29              ARP               0.001000               0.998000\n",
      "30              UDP               0.000667               0.998667\n",
      "31  fin_flag_number               0.000333               0.999000\n",
      "32  syn_flag_number               0.000333               0.999333\n",
      "33              IPv               0.000333               0.999667\n",
      "43  cwr_flag_number               0.000000               1.000000\n",
      "42  ece_flag_number               0.000000               1.000000\n",
      "41              DNS               0.000000               1.000000\n",
      "40            Drate               0.000000               1.000000\n",
      "39           Telnet               0.000000               1.000000\n",
      "34              TCP               0.000333               1.000000\n",
      "37              IRC               0.000000               1.000000\n",
      "36             DHCP               0.000000               1.000000\n",
      "35              LLC               0.000000               1.000000\n",
      "44            Srate               0.000000               1.000000\n",
      "38              SSH               0.000000               1.000000\n",
      "45             SMTP               0.000000               1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anubhav\\AppData\\Local\\Temp\\ipykernel_24388\\3633410587.py:31: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  record_collinear = pd.concat([record_collinear, temp_df], ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "df1 = Auto_Feature_Engineering(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
