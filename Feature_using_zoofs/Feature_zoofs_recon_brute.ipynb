{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from zoofs import ParticleSwarmOptimization\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss, accuracy_score, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(r'E:\\Projects\\Capstone\\Final\\Data\\Spoofing_Brute.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].apply(lambda x: 0 if 'Brute' in str(x) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['label'],axis=1)\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2, shuffle=True,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_topass(model,X_train, y_train, X_valid, y_valid):      \n",
    "    model.fit(X_train,y_train)  \n",
    "    P = log_loss(y_valid,model.predict_proba(X_valid))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m [ 2024-12-19 14:50:47,184 ] \u001b[0mFinished iteration #0 with objective value -0.9577768217633197. Current best value is -0.9577768217633197 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:51:01,865 ] \u001b[0mFinished iteration #1 with objective value -0.9602780108768456. Current best value is -0.9602780108768456 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:51:16,738 ] \u001b[0mFinished iteration #2 with objective value -0.9597766520581192. Current best value is -0.9602780108768456 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:51:31,253 ] \u001b[0mFinished iteration #3 with objective value -0.9612789191868389. Current best value is -0.9612789191868389 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:51:46,772 ] \u001b[0mFinished iteration #4 with objective value -0.9619933302719527. Current best value is -0.9619933302719527 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:52:02,009 ] \u001b[0mFinished iteration #5 with objective value -0.960564147563222. Current best value is -0.9619933302719527 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:52:17,399 ] \u001b[0mFinished iteration #6 with objective value -0.9619221461780713. Current best value is -0.9619933302719527 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:52:32,403 ] \u001b[0mFinished iteration #7 with objective value -0.9616360376315883. Current best value is -0.9619933302719527 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:52:47,971 ] \u001b[0mFinished iteration #8 with objective value -0.9611353199946114. Current best value is -0.9619933302719527 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:53:05,250 ] \u001b[0mFinished iteration #9 with objective value -0.9627797633449068. Current best value is -0.9627797633449068 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:53:20,675 ] \u001b[0mFinished iteration #10 with objective value -0.9617077385363312. Current best value is -0.9627797633449068 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:53:35,627 ] \u001b[0mFinished iteration #11 with objective value -0.960847432194773. Current best value is -0.9627797633449068 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:53:50,889 ] \u001b[0mFinished iteration #12 with objective value -0.9614928014700826. Current best value is -0.9627797633449068 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:54:06,271 ] \u001b[0mFinished iteration #13 with objective value -0.9628494082918158. Current best value is -0.9628494082918158 \u001b[0m\n",
      "\u001b[32m [ 2024-12-19 14:54:21,526 ] \u001b[0mFinished iteration #14 with objective value -0.9612790938184542. Current best value is -0.9628494082918158 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['flow_duration',\n",
       " 'Header_Length',\n",
       " 'Duration',\n",
       " 'Rate',\n",
       " 'Srate',\n",
       " 'syn_flag_number',\n",
       " 'rst_flag_number',\n",
       " 'psh_flag_number',\n",
       " 'ack_flag_number',\n",
       " 'syn_count',\n",
       " 'urg_count',\n",
       " 'rst_count',\n",
       " 'HTTPS',\n",
       " 'DNS',\n",
       " 'Telnet',\n",
       " 'SMTP',\n",
       " 'IRC',\n",
       " 'TCP',\n",
       " 'UDP',\n",
       " 'IPv',\n",
       " 'LLC',\n",
       " 'Tot_sum',\n",
       " 'Max',\n",
       " 'IAT',\n",
       " 'Magnitue',\n",
       " 'Weight']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def objective_function_topass(model, X_train, y_train, X_valid, y_valid):      \n",
    "    \"\"\"\n",
    "    Objective function for optimization. Trains the model and evaluates F1 score.\n",
    "    Returns the negative F1 score to maximize it.\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)  \n",
    "    P = f1_score(y_valid, y_pred, average='weighted')  # Weighted F1 score\n",
    "    return -P  # Negative F1 score to maximize it\n",
    "\n",
    "# Assuming ParticleSwarmOptimization is implemented and imported\n",
    "algo_object = ParticleSwarmOptimization(\n",
    "    objective_function_topass,\n",
    "    n_iteration=15,\n",
    "    population_size=30,\n",
    "    minimize=True\n",
    ")\n",
    "\n",
    "# Replace LightGBM model with XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    eval_metric=\"logloss\"  # Evaluation metric for XGBoost\n",
    ")\n",
    "\n",
    "# Fit the optimization algorithm\n",
    "algo_object.fit(xgb_model, X_train, y_train, X_test, y_test, verbose=True)\n",
    "\n",
    "# Plot your results (if your ParticleSwarmOptimization class supports it)\n",
    "# algo_object.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_topass(model, X_train, y_train, X_valid, y_valid):      \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)  \n",
    "    P = f1_score(y_valid, y_pred)\n",
    "    return -P  # Return negative F1 score to maximize it\n",
    "\n",
    "    \n",
    "# create object of algorithm\n",
    "algo_object_pso=ParticleSwarmOptimization(objective_function_topass,n_iteration=15,\n",
    "                                    population_size=30,minimize=True)\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier()                                       \n",
    "# fit the algorithm\n",
    "algo_object_pso.fit(lgb_model,X_train, y_train, X_test, y_test,verbose=True)\n",
    "#plot your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_xg = algo_object.best_feature_list\n",
    "feat_lg = algo_object_pso.best_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_object.plot_history()\n",
    "algo_object_pso.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "clf.fit(X_train[feat_xg], y_train)\n",
    "y_pred=clf.predict(X_test[feat_xg])\n",
    "\n",
    "accuracy=accuracy_score(y_pred, y_test)\n",
    "print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "clf.fit(X_train[feat_lg], y_train)\n",
    "y_pred=clf.predict(X_test[feat_lg])\n",
    "\n",
    "accuracy=accuracy_score(y_pred, y_test)\n",
    "print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoofs import HarrisHawkOptimization\n",
    "\n",
    "def objective_function_topass_harris(model,X_train, y_train, X_valid, y_valid):      \n",
    "    model.fit(X_train,y_train)  \n",
    "    P=f1_score(y_valid,model.predict_proba(X_valid))\n",
    "    return -P\n",
    "\n",
    "# import an algorithm !  \n",
    "\n",
    "\n",
    "# create object of algorithm\n",
    "algo_object_haris=HarrisHawkOptimization(objective_function_topass,\n",
    "                                   n_iteration=10,\n",
    "                                   population_size=30,\n",
    "                                   minimize=True)\n",
    "\n",
    "# fit the algorithm\n",
    "algo_object_haris.fit(lgb_model,X_train, y_train, X_test, y_test,verbose=True)\n",
    "\n",
    "#plot your results\n",
    "algo_object_haris.plot_history()\n",
    "\n",
    "# extract the best  feature set\n",
    "algo_object_haris.best_feature_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_object_haris.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_object_haris.best_feature_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_hawk = algo_object_haris.best_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "clf.fit(X_train[feat_hawk], y_train)\n",
    "y_pred=clf.predict(X_test[feat_hawk])\n",
    "\n",
    "accuracy=accuracy_score(y_pred, y_test)\n",
    "print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
